{"cells":[{"cell_type":"markdown","metadata":{"id":"zG-Qw6fZkor6"},"source":["# **Fine-tuning**\n","\n","Le fine-tuning permet d'entraîner un large modèle de langue tels que GPT3.5 sur des données spécifiques.\n","\n","\n","Le réglage fin améliore l'apprentissage en quelques étapes en s'entraînant sur beaucoup plus d'exemples que ne peut en contenir l'invite, vous permettant d'obtenir de meilleurs résultats sur un grand nombre de tâches. Une fois qu'un modèle a été affiné, vous n'aurez plus besoin de fournir autant d'exemples dans l'invite. Cela permet de réduire les coûts et de permettre des requêtes à faible latence.\n","\n","Le réglage fin vous permet de tirer le meilleur parti des modèles disponibles via l'API en fournissant :\n","\n","1. Des résultats de meilleure qualité que les invites\n","2. Capacité à s'entraîner sur plus d'exemples que ne peut en contenir une invite\n","3. Économies de jetons grâce à des invites plus courtes\n","4. Demandes de latence réduites\n","\n","\n","<img src='https://images.openai.com/blob/d8d965fd-e54c-4c9b-9d7d-b5d9910d38b2/gpt-3-5-turbo-fine-tuning-and-api-updates.png?trim=0,0,0,0&width=3200'>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_WH8uobZkor8"},"source":["**Les modèles supportés par le fine-tuning**\n","\n","Un réglage fin est actuellement disponible pour les modèles suivants :\n","\n","- `gpt-3.5-turbo-0613`(recommandé)\n","- `babbage-002`\n","- `davinci-002`\n"]},{"cell_type":"markdown","metadata":{"id":"WzZ2-7qVkor8"},"source":["**Tarification**\n","\n","Les coûts de réglage fin sont répartis en deux catégories : le coût de formation initial et le coût d'utilisation :\n","\n","- Formation : 0,008 $ / 1 000 tokens\n","\n","- Entrée d'utilisation : 0,012 $ / 1 000 tokens\n","\n","- Sortie d'utilisation : 0,016 $ / 1 000 tokens\n","\n","Par exemple, un gpt-3.5-turbotravail de mise au point avec un fichier de formation de 100 000 jetons formés pendant 3 époques aurait un coût attendu de 2,40 $.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VOAvHCXukor8"},"source":["**Cas d'utilisation courants**\n","\n","Quelques cas d'utilisation courants dans lesquels un réglage fin peut améliorer les résultats :\n","\n","- Définir le style, le ton, le format ou d'autres aspects qualitatifs\n","- Améliorer la fiabilité pour produire le résultat souhaité\n","- Correction des échecs de suivi d'invites complexes\n","- Gérer de nombreux cas extrêmes de manière spécifique\n","- Exécuter une nouvelle compétence ou une nouvelle tâche difficile à articuler dans une invite\n","\n","Une manière générale d'envisager ces cas consiste à considérer qu'il est plus facile de « montrer, pas de dire ». Dans les sections à venir, nous explorerons comment configurer les données pour un réglage fin et divers exemples dans lesquels le réglage fin améliore les performances par rapport au modèle de base."]},{"cell_type":"markdown","metadata":{"id":"v5XLevv0kor8"},"source":["**Méthodologie**\n","\n","Le réglage fin améliore l'apprentissage en quelques étapes en s'entraînant sur beaucoup plus d'exemples que ne peut en contenir l'invite, vous permettant d'obtenir de meilleurs résultats sur un grand nombre de tâches. Une fois qu'un modèle a été affiné, vous n'aurez plus besoin de fournir autant d'exemples dans l'invite. Cela permet de réduire les coûts et de permettre des requêtes à faible latence.\n","\n","À un niveau élevé, le réglage fin implique les étapes suivantes :\n","\n","1. Préparer et télécharger les données d'entraînement\n","2. Former un nouveau modèle affiné\n","3. Utilisez votre modèle affiné\n"]},{"cell_type":"markdown","metadata":{"id":"OwkHbyxOkor9"},"source":["### **1. Préparer et télécharger les données d'entraînement**\n","\n","Le réglage fin nécessite un fichier de formation au format JSONL. Chaque ligne du fichier doit contenir un objet JSON avec les clés/valeurs suivantes :\n","\n","- Pour les modèles `ChatCompletion` gt3.5-turbo-0613 :\n","\n","```{\n","    \"message\":[\n","        {\n","            \"role\":\"system\",\n","            \"content\":\"contenu du message\"\n","        },\n","        {\n","            \"role\":\"user\",\n","            \"content\":\"contenu du message\"\n","        },\n","        {\n","            \"role\":\"assistant\",\n","            \"content\":\"contenu du message\"\n","        }\n","    ]\n","}\n","```\n","\n","- Pour les modèles `TextGeneration` :\n","```\n","{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n","```\n","\n","`**Attention**` : **Chaque message doit être placé sur une ligne dans le fichier JSONL. Les messages peuvent être de n'importe quelle longueur.**\n","\n","**Au moins 10 messages sont nécessaires** pour entraîner un modèle. Pour obtenir de meilleurs résultats, nous recommandons d'utiliser au moins 100 messages. Pour les modèles de génération de texte, nous recommandons d'utiliser au moins 1000 messages.\n","\n","Le réglage fin est disponible à partir de la version 0.28.0 de l'[API d'OpenAI](https://platform.openai.com/docs/guides/fine-tuning)."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"rIhDExEmkor9","outputId":"4103de76-3f06-42ab-e90b-13b5bef36b5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openai==0.28.0\n","  Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n","Requirement already satisfied: requests>=2.20 in c:\\python311\\lib\\site-packages (from openai==0.28.0) (2.31.0)\n","Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from openai==0.28.0) (4.66.1)\n","Requirement already satisfied: aiohttp in c:\\python311\\lib\\site-packages (from openai==0.28.0) (3.8.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (2023.7.22)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\python311\\lib\\site-packages (from aiohttp->openai==0.28.0) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python311\\lib\\site-packages (from aiohttp->openai==0.28.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python311\\lib\\site-packages (from aiohttp->openai==0.28.0) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python311\\lib\\site-packages (from aiohttp->openai==0.28.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\python311\\lib\\site-packages (from aiohttp->openai==0.28.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\python311\\lib\\site-packages (from aiohttp->openai==0.28.0) (1.3.1)\n","Requirement already satisfied: colorama in c:\\users\\admin-stagiaire\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->openai==0.28.0) (0.4.6)\n","Installing collected packages: openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 0.27.0\n","    Uninstalling openai-0.27.0:\n","      Successfully uninstalled openai-0.27.0\n","Successfully installed openai-0.28.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.3 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install openai==0.28.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5fFzoSPkor-"},"outputs":[],"source":["# Structure des données pour l'entrainement d'un modèle ChatCompletion\n","# Moèle concerné : gpt-3.5-turbo-0613 :\n","\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What is fine-tuning in machine learning?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning in machine learning involves training a pre-existing model on a specific task or dataset to make it more accurate or suitable for that task. It's like giving the model specialized knowledge.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Why is fine-tuning important for AI models?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning is crucial because it allows AI models to adapt to specific applications or domains, improving their performance and making them more useful in real-world scenarios.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Can you explain the steps involved in fine-tuning a language model?\"}, {\"role\": \"assistant\", \"content\": \"Certainly! Fine-tuning typically includes preparing training data, training the model on that data, and then using the fine-tuned model for your specific tasks. It's a process of specialization.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What models can be fine-tuned for language tasks?\"}, {\"role\": \"assistant\", \"content\": \"Currently, you can fine-tune models like gpt-3.5-turbo-0613, babbage-002, and davinci-002. These models can be adapted to various language-related tasks.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"When should I consider using fine-tuning for AI models?\"}, {\"role\": \"assistant\", \"content\": \"You should consider fine-tuning when you need to tailor a model for specific tasks or improve its performance on certain applications. It's a powerful tool for customization.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How can fine-tuning benefit natural language processing (NLP) tasks?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning can make NLP models more accurate and contextually aware for tasks like text generation, translation, sentiment analysis, and more. It's all about making the model 'smarter' in a particular domain.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What is the role of training data in fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"Training data is essential for fine-tuning as it helps the model learn from examples. You need a diverse set of conversations or text that reflects the scenarios you want the model to handle.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How many training examples should I provide for fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"You should provide at least 10 training examples, but for optimal results, it's often recommended to have between 50 and 100 examples. The right number depends on your specific use case.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What are the token limits for fine-tuning examples?\"}, {\"role\": \"assistant\", \"content\": \"Each fine-tuning example is limited to 4096 tokens. Make sure the total token count in the messages' content is under 4,000 to fit within this limit.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How can I estimate the costs of fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"You can estimate the costs by using this formula: base cost per 1,000 tokens * number of tokens in the input file * number of training epochs. This will give you an idea of the expected cost for your fine-tuning job.\"}]}\n","\n","\n","# Structure des données pour l'entrainement d'un modèle Completion\n","# Moèle concerné : babbage-002 et davinci-002\n","\n","{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n","{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n","{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"]},{"cell_type":"markdown","metadata":{"id":"9AefxRSlkor_"},"source":["### **2. Former un nouveau modèle affiné**\n","\n","Une fois les instructions de formation préparées et placées dans un fichier `data.jsonl`, vous pouvez commencer à former votre modèle affiné en préparant les données d'entrainement puis en lançant la formation."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"WjOPxX5_kor_","outputId":"7ab3cb42-72d6-4722-e1c0-ac5a57fe74be"},"outputs":[{"data":{"text/plain":["<File file id=file-b5Sn25QzdzTrkOFEhnokhRbw at 0x1eaa49ea870> JSON: {\n","  \"object\": \"file\",\n","  \"id\": \"file-b5Sn25QzdzTrkOFEhnokhRbw\",\n","  \"purpose\": \"fine-tune\",\n","  \"filename\": \"file\",\n","  \"bytes\": 4085,\n","  \"created_at\": 1695369195,\n","  \"status\": \"uploaded\",\n","  \"status_details\": null\n","}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import openai\n","\n","\n","# Préparation des données pour l'entrainement d'un modèle ChatCompletion\n","openai.api_key = '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'\n","response = openai.File.create(\n","  file=open(\"./data.jsonl\", \"rb\"),\n","  purpose='fine-tune'\n",")\n","response"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"oNobgKd7kor_","outputId":"df3bba55-a7bd-4848-85c7-b0dd62ecbadc"},"outputs":[{"data":{"text/plain":["<FineTuningJob fine_tuning.job id=ftjob-IOXUBhVarK7t6EDPNVyIRina at 0x1eaa49eac30> JSON: {\n","  \"object\": \"fine_tuning.job\",\n","  \"id\": \"ftjob-IOXUBhVarK7t6EDPNVyIRina\",\n","  \"model\": \"gpt-3.5-turbo-0613\",\n","  \"created_at\": 1695369202,\n","  \"finished_at\": null,\n","  \"fine_tuned_model\": null,\n","  \"organization_id\": \"org-7z5FszmIViA0Ipk6dzVjcojw\",\n","  \"result_files\": [],\n","  \"status\": \"validating_files\",\n","  \"validation_file\": null,\n","  \"training_file\": \"file-b5Sn25QzdzTrkOFEhnokhRbw\",\n","  \"hyperparameters\": {\n","    \"n_epochs\": \"auto\"\n","  },\n","  \"trained_tokens\": null,\n","  \"error\": null\n","}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Entrainement du modèle ChatCompletion sur les serveurs d'OpenAI\n","response_train_model = openai.FineTuningJob.create(\n","    training_file=response['id'],\n","    model=\"gpt-3.5-turbo\",\n","    )\n","\n","response_train_model"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"nYIRGJs2kor_","outputId":"77a64dfb-e880-417d-a19c-8f115e8bd1fe"},"outputs":[{"data":{"text/plain":["<OpenAIObject list at 0x1eaa49eae10> JSON: {\n","  \"object\": \"list\",\n","  \"data\": [\n","    {\n","      \"object\": \"fine_tuning.job\",\n","      \"id\": \"ftjob-IOXUBhVarK7t6EDPNVyIRina\",\n","      \"model\": \"gpt-3.5-turbo-0613\",\n","      \"created_at\": 1695369202,\n","      \"finished_at\": null,\n","      \"fine_tuned_model\": null,\n","      \"organization_id\": \"org-7z5FszmIViA0Ipk6dzVjcojw\",\n","      \"result_files\": [],\n","      \"status\": \"validating_files\",\n","      \"validation_file\": null,\n","      \"training_file\": \"file-b5Sn25QzdzTrkOFEhnokhRbw\",\n","      \"hyperparameters\": {\n","        \"n_epochs\": \"auto\"\n","      },\n","      \"trained_tokens\": null,\n","      \"error\": null\n","    },\n","    {\n","      \"object\": \"fine_tuning.job\",\n","      \"id\": \"ftjob-gH7nMA3Ycw0SUP35wRcfxDaT\",\n","      \"model\": \"gpt-3.5-turbo-0613\",\n","      \"created_at\": 1695367207,\n","      \"finished_at\": 1695367532,\n","      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:dawan::81UpZvUs\",\n","      \"organization_id\": \"org-7z5FszmIViA0Ipk6dzVjcojw\",\n","      \"result_files\": [\n","        \"file-9rOSyUxbhoxp5bOZeueVECfm\"\n","      ],\n","      \"status\": \"succeeded\",\n","      \"validation_file\": null,\n","      \"training_file\": \"file-Y8SkMTKJUSS8e2sCZi6UXBx0\",\n","      \"hyperparameters\": {\n","        \"n_epochs\": 10\n","      },\n","      \"trained_tokens\": 7650,\n","      \"error\": null\n","    },\n","    {\n","      \"object\": \"fine_tuning.job\",\n","      \"id\": \"ftjob-eKaoC2JvMszRkgXGIZTNWTWb\",\n","      \"model\": \"gpt-3.5-turbo-0613\",\n","      \"created_at\": 1695360336,\n","      \"finished_at\": 1695360590,\n","      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:dawan::81T1cm2H\",\n","      \"organization_id\": \"org-7z5FszmIViA0Ipk6dzVjcojw\",\n","      \"result_files\": [\n","        \"file-pNub2cZ9QSkE1QyL1BJhc828\"\n","      ],\n","      \"status\": \"succeeded\",\n","      \"validation_file\": null,\n","      \"training_file\": \"file-ZKPK5woFgyzaqWYEw16rnM7I\",\n","      \"hyperparameters\": {\n","        \"n_epochs\": 10\n","      },\n","      \"trained_tokens\": 7650,\n","      \"error\": null\n","    },\n","    {\n","      \"object\": \"fine_tuning.job\",\n","      \"id\": \"ftjob-Gw4ptSpbw4lfNxBEAIo05WzL\",\n","      \"model\": \"gpt-3.5-turbo-0613\",\n","      \"created_at\": 1695300171,\n","      \"finished_at\": 1695300435,\n","      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:dawan::81DNLWeI\",\n","      \"organization_id\": \"org-7z5FszmIViA0Ipk6dzVjcojw\",\n","      \"result_files\": [\n","        \"file-g9xgyuZRh0XyuOV5SD4kNxBL\"\n","      ],\n","      \"status\": \"succeeded\",\n","      \"validation_file\": null,\n","      \"training_file\": \"file-1eQ9g62RVz5qNRuBOIKKCHun\",\n","      \"hyperparameters\": {\n","        \"n_epochs\": 10\n","      },\n","      \"trained_tokens\": 7650,\n","      \"error\": null\n","    },\n","    {\n","      \"object\": \"fine_tuning.job\",\n","      \"id\": \"ftjob-il0rwjltbIbfYodQU00Nklh9\",\n","      \"model\": \"babbage-002\",\n","      \"created_at\": 1695224012,\n","      \"finished_at\": 1695224089,\n","      \"fine_tuned_model\": \"ft:babbage-002:dawan::80tVzobQ\",\n","      \"organization_id\": \"org-7z5FszmIViA0Ipk6dzVjcojw\",\n","      \"result_files\": [\n","        \"file-7gMaRsY5SLc0bbtA6his1Bd8\"\n","      ],\n","      \"status\": \"succeeded\",\n","      \"validation_file\": null,\n","      \"training_file\": \"file-8LwfufpmLNrik9HrxxOHFSbe\",\n","      \"hyperparameters\": {\n","        \"n_epochs\": 8\n","      },\n","      \"trained_tokens\": 1056,\n","      \"error\": null\n","    }\n","  ],\n","  \"has_more\": false\n","}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["openai.FineTuningJob.list()"]},{"cell_type":"markdown","metadata":{"id":"M7bI21Qzkor_"},"source":["### **3. Utilisez votre modèle affiné**\n","\n","Une fois que votre modèle est formé, vous pouvez l'utiliser pour générer des résultats en utilisant l'API comme vous le feriez avec n'importe quel autre modèle. Il est possible de visualiser la courbe d'apprentissage de votre modèle dans le tableau de bord de votre compte :\n","https://platform.openai.com/finetune\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"hMezveRqkor_","outputId":"246eb2ef-05a2-477d-a579-32a07d5ecb6b"},"outputs":[{"data":{"text/plain":["['curie-search-query',\n"," 'babbage-search-document',\n"," 'text-search-babbage-doc-001',\n"," 'babbage',\n"," 'text-babbage-001',\n"," 'text-similarity-davinci-001',\n"," 'davinci',\n"," 'davinci-similarity',\n"," 'code-davinci-edit-001',\n"," 'curie-similarity',\n"," 'code-search-babbage-code-001',\n"," 'curie-instruct-beta',\n"," 'text-search-ada-doc-001',\n"," 'davinci-instruct-beta',\n"," 'text-search-davinci-doc-001',\n"," 'text-curie-001',\n"," 'davinci-search-query',\n"," 'text-similarity-curie-001',\n"," 'text-davinci-001',\n"," 'text-search-davinci-query-001',\n"," 'text-davinci-003',\n"," 'ada-search-document',\n"," 'ada-code-search-code',\n"," 'babbage-002',\n"," 'whisper-1',\n"," 'gpt-4-0613',\n"," 'gpt-4',\n"," 'davinci-002',\n"," 'davinci-search-document',\n"," 'curie-search-document',\n"," 'babbage-code-search-text',\n"," 'babbage-code-search-code',\n"," 'babbage-search-query',\n"," 'text-search-ada-query-001',\n"," 'code-search-ada-text-001',\n"," 'gpt-3.5-turbo-instruct-0914',\n"," 'gpt-3.5-turbo-instruct',\n"," 'gpt-3.5-turbo-0613',\n"," 'ada-search-query',\n"," 'ada-code-search-text',\n"," 'text-search-curie-query-001',\n"," 'text-davinci-002',\n"," 'code-search-babbage-text-001',\n"," 'text-embedding-ada-002',\n"," 'text-davinci-edit-001',\n"," 'ada',\n"," 'text-ada-001',\n"," 'ada-similarity',\n"," 'code-search-ada-code-001',\n"," 'text-similarity-babbage-001',\n"," 'text-similarity-ada-001',\n"," 'gpt-3.5-turbo-0301',\n"," 'curie',\n"," 'babbage-similarity',\n"," 'gpt-3.5-turbo-16k',\n"," 'gpt-3.5-turbo',\n"," 'gpt-4-0314',\n"," 'text-search-babbage-query-001',\n"," 'text-search-curie-doc-001',\n"," 'gpt-3.5-turbo-16k-0613',\n"," 'ft:babbage-002:dawan::80tVzobQ',\n"," 'ft:gpt-3.5-turbo-0613:dawan::81T1cm2H',\n"," 'ft:gpt-3.5-turbo-0613:dawan::81DNLWeI',\n"," 'ft:gpt-3.5-turbo-0613:dawan::81UpZvUs']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["[model['id'] for model in openai.Model().list()[\"data\"]]"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"oIGI84mokor_","outputId":"b4697bfd-b33d-40cc-cd3c-e35cd4ce9c54"},"outputs":[{"data":{"text/plain":["\"Fine-tuning in machine learning involves training a pre-existing model on a specific task or dataset to make it more accurate or suitable for that task. It's like giving the model specialized knowledge.\""]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Utilisation du modèle fine-tuné\n","reponse = openai.ChatCompletion.create(\n","    model='ft:gpt-3.5-turbo-0613:dawan::81DNLWeI',\n","    messages=[\n","        {\"role\": \"system\",\"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n","        {\"role\": \"user\",\n","         \"content\":\"What is fine-tuning in machine learning?\"},\n","    ],\n",")\n","\n","reponse['choices'][0]['message'][\"content\"]"]},{"cell_type":"markdown","metadata":{"id":"S0IlPa8rkor_"},"source":["___\n","\n","# Exercice\n","\n"]},{"cell_type":"markdown","metadata":{"id":"268S5ldekor_"},"source":["1. A L'aide de ChatGPT, créez un fichier de données d'entrainement au format JSONL contenant au moins 10 messages puis placez le dans un fichier `data.jsonl` Entrainez ensuite un modèle de chatbot sur un domaine de votre choix.\n","**Astuce** : Copiez le contenu de votre choix (exemple : une page internet) puis demandez à GhatGPT de générer le fichier de données d'entrainement au format JSONL.\n","\n","2. Créez une méthode `openai_chat_finetune()` qui prend en paramètre le nom du fichier de données d'entrainement et retourne le nom du modèle entrainé."]},{"cell_type":"markdown","metadata":{"id":"uiEvJa8ykosA"},"source":["### **Exemple de prompt à envoyer à ChatGPT**\n","\n","```\n","\n","A partir du document suivant crée-moi un fichier jsonl contentant des exemples d'échange entre un chatbot et un utilisateur qui sera utilisé pour le fine-tuning d'un model gpt3 pour répondre aux questions de développeurs qui souhaite comprendre comment fonctionne le finetuning. Le jsonl aura la forme suivante :\n","\n","{\"messages\": [{\"role\": \"system\", \"content\": \"definir le rol system\"}, {\"role\": \"user\", \"content\": \"question type posé par un utilisateur\"}, {\"role\": \"assistant\", \"content\": \"Réponse de l'assistant\"}]}\n","\n","\n","Le modèle du fichier json doit avoir cette forme :\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n","\n","\n","Propose moi à nouveau 20 exemples d'intéraction entre un assistant et un utilisateur au sujet du finetuning en respectant le modèle donné.\n","\n","Voici la documentation à partir de laquelle tu dois créer les exemples : à compléter\n","\n","```\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","c:\\Python311\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n","  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"]},{"ename":"InvalidRequestError","evalue":"invalid training_file: ./data.jsonl","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Admin-Stagiaire\\Desktop\\Formation Master\\Formation_IA\\6. Fine-tuning.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin-Stagiaire/Desktop/Formation%20Master/Formation_IA/6.%20Fine-tuning.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m TextProcessor\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin-Stagiaire/Desktop/Formation%20Master/Formation_IA/6.%20Fine-tuning.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m text_processor \u001b[39m=\u001b[39m TextProcessor()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin-Stagiaire/Desktop/Formation%20Master/Formation_IA/6.%20Fine-tuning.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trained_model_name \u001b[39m=\u001b[39m text_processor\u001b[39m.\u001b[39;49mopenai_chat_finetune(\u001b[39m\"\u001b[39;49m\u001b[39m./data.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin-Stagiaire/Desktop/Formation%20Master/Formation_IA/6.%20Fine-tuning.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m trained_model_name:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin-Stagiaire/Desktop/Formation%20Master/Formation_IA/6.%20Fine-tuning.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModèle entraîné avec succès : \u001b[39m\u001b[39m{\u001b[39;00mtrained_model_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Admin-Stagiaire\\Desktop\\Formation Master\\Formation_IA\\functions.py:229\u001b[0m, in \u001b[0;36mTextProcessor.openai_chat_finetune\u001b[1;34m(self, training_data_file)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopenai_chat_finetune\u001b[39m(\u001b[39mself\u001b[39m, training_data_file):\n\u001b[1;32m--> 229\u001b[0m     training_job \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mFineTuningJob\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m    230\u001b[0m         training_file\u001b[39m=\u001b[39;49mtraining_data_file,\n\u001b[0;32m    231\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    233\u001b[0m     training_job\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m training_job\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m:\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_resources\\abstract\\createable_api_resource.py:57\u001b[0m, in \u001b[0;36mCreateableAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m     40\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m     48\u001b[0m ):\n\u001b[0;32m     49\u001b[0m     requestor, url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__prepare_create_requestor(\n\u001b[0;32m     50\u001b[0m         api_key,\n\u001b[0;32m     51\u001b[0m         api_base,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m         organization,\n\u001b[0;32m     55\u001b[0m     )\n\u001b[1;32m---> 57\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m     58\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params, request_id\u001b[39m=\u001b[39;49mrequest_id\n\u001b[0;32m     59\u001b[0m     )\n\u001b[0;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39mconvert_to_openai_object(\n\u001b[0;32m     62\u001b[0m         response,\n\u001b[0;32m     63\u001b[0m         api_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         plain_old_data\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mplain_old_data,\n\u001b[0;32m     67\u001b[0m     )\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n","\u001b[1;31mInvalidRequestError\u001b[0m: invalid training_file: ./data.jsonl"]}],"source":["from functions import TextProcessor\n","\n","text_processor = TextProcessor()\n","trained_model_name = text_processor.openai_chat_finetune(\"./data.jsonl\")\n","\n","if trained_model_name:\n","    print(f\"Modèle entraîné avec succès : {trained_model_name}\")\n","else:\n","    print(\"L'entraînement a échoué.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
