{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement du langage naturel\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Qu'est-ce que le traitement automatique du langage naturel ?\n",
    "\n",
    "Le traitement du langage naturel (NLP en anglais) est un champ de l'informatique, de l'intelligence artificielle et de la linguistique qui traite des interactions entre les ordinateurs et les langues humaines (naturelles). Le but du NLP est de permettre aux ordinateurs de comprendre, d'interpr√©ter et de g√©n√©rer des langues humaines.\n",
    "\n",
    "Les applications du NLP incluent :\n",
    "- la classification de textes, \n",
    "- l'analyse de sentiments, \n",
    "- la traduction de langues,\n",
    "- la correction grammaticale \n",
    "- la reconnaissance d'entit√©s nomm√©es, \n",
    "- la reconnaissance vocale,\n",
    "- les chatbots. \n",
    "\n",
    "\n",
    "Nous avons de plus en plus l'habitude d'√™tre en interaction avec cette branche de l'IA notamment via :\n",
    "\n",
    "üëâüèª **les outils de correction en ligne tels que [Scribens](https://www.scribens.fr/), [reverso](https://www.reverso.net/orthographe/correcteur-francais/) ou bien les correcteurs orthographiques des messageries t√©l√©phoniques**,\n",
    "<img src='https://www.barometre-entreprendre.fr/wp-content/uploads/2022/05/les-avantages-de-scribens.png'>\n",
    "\n",
    "üëâüèª **les propositions de r√©ponse automatique telle que le propose Google dans son outil de messagerie Gmail**,\n",
    "<img src=''>\n",
    "\n",
    "üëâüèª **les applications de traduction tel que Google Translate**,\n",
    "<img src='https://lh3.googleusercontent.com/EEZl-eNxoEik6MTLsK9BFtfKrsNVOy7lnNq3DS4Db9Qn3l9F68gNZHvrqeHFeLB-_d8qi9a0ZgYjYh3MCvB3mB0uh-oH0F5IYyYYC5grS1iGUd1z7oLuXV0dqMV9CuWSvV3OUPwz'>\n",
    "\n",
    "üëâüèª **les chatbots de relation client pr√©sents sur certains sites internet**,\n",
    "<img src='https://offreduweb.com/wp-content/uploads/5561/chatbot-le-robot-5f02378e4438b.png'>\n",
    "\n",
    "\n",
    "üëâüèª **les assistants vocaux tels qu'Alexa, Cortana, Google**,\n",
    "<img src='https://drive.google.com/uc?export=view&id=1MBtYT8Rd-ga18bwcQqbqI1atmC7HhkgD'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Les techniques de NLP utilisent des algorithmes d'apprentissage automatique, \n",
    "- tels que les arbres de d√©cision, \n",
    "- les for√™ts al√©atoires, \n",
    "- les r√©seaux de neurones et l'apprentissage profond, pour analyser et mod√©liser la structure et la signification des langues.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Les bases de python √† conna√Ætre pour faire du NLP\n",
    "\n",
    "<img src='https://miro.medium.com/max/1200/1*Pl9OyXXrLH_5JiQB8vNx3w.jpeg'>\n",
    "\n",
    "La premi√®re partie de cette formation sera ce consentrera sur les fondamentaux du NLP ainsi que les bases √† connaitre en python pour manipuler du contenu textuel.\n",
    "\n",
    "\n",
    "### 2.1 Les fonctions int√©gr√©es\n",
    "\n",
    "- `print()` : permet d'afficher du texte ou des valeurs sur la sortie standard, g√©n√©ralement la console.\n",
    "- `input()` : lit une entr√©e de l'utilisateur et la retourne sous forme de cha√Æne de caract√®res.\n",
    "- `len()` : cette fonction retourne la longueur d'un objet, comme une liste, un tableau ou une cha√Æne de caract√®res.\n",
    "- `type()` : cette fonction retourne le type d'un objet.\n",
    "- `dir()` : retourne une liste des noms d'attributs et de m√©thodes associ√©s √† un objet donn√©. \n",
    "- `range()` :  retourne une s√©quence d'entiers, g√©n√©r√©e en fonction d'un d√©but, d'une fin et d'un pas donn√©s.\n",
    "- `str()`, `int()`,`float()`,`bool()` , `list()`, `dict()`, `tuple()` : **data type** permettant √©galement des objets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Liste compr√©hension et conditions\n",
    "\n",
    "Une liste compr√©hension est une mani√®re concise et lisible  de cr√©er des listes √† partir d'autres listes. Une liste compr√©hension est √©crite en utilisant une expression simplifi√©e qui d√©crit comment les √©l√©ments doivent √™tre transform√©s. Cela permet de g√©n√©rer des listes plus rapidement et avec moins de code qu'en utilisant des boucles classiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 16, 36, 64, 100]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cr√©ation d'une liste de mani√®re traditionnelle avec une boucle for\n",
    "list_carre = []\n",
    "\n",
    "for x in range(11):\n",
    "    if x % 2 == 0:\n",
    "        list_carre.append(x**2)\n",
    "\n",
    "list_carre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 16, 36, 64, 100]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liste compr√©hension : Carr√© des nombres paires jusqu'√† 10\n",
    "list_carre = [x**2 for x in range(11) if x % 2 == 0]\n",
    "list_carre"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Programation ori√©t√©e Objet\n",
    "\n",
    "La programmation orient√©e objet en Python est un paradigme de programmation qui utilise des objets et des classes pour structurer le code. \n",
    "\n",
    "Les objets sont des **instances de classes**, possedant des atributs et des m√©thodes. Les attributs sont des variables de classe, tandis que les m√©thodes sont des fonctions de classe. \n",
    "\n",
    "Les classes peuvent h√©riter les propri√©t√©s et les m√©thodes d'autres classes, ce qui permet de cr√©er des relations de parent√© entre les objets. En utilisant la programmation orient√©e objet, on peut cr√©er des programmes plus organis√©s et plus faciles √† maintenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©claration d'une classe, param√®tre 1 et 2 seront\n",
    "\n",
    "class NomDeLaClasse:\n",
    "  # D√©finition des attributs \n",
    "  def __init__(self, parametre1, parametre2):\n",
    "    self.parametre1 = parametre1\n",
    "    self.parametre2 = parametre2\n",
    "\n",
    "  # D√©finition d'une m√©thode\n",
    "  def nom_de_la_methode(self, p1, p2):\n",
    "    pass\n",
    "\n",
    "# Instance de classe\n",
    "objet = NomDeLaClasse('parametre1', 'parametre2')\n",
    "\n",
    "\n",
    "#H√©ritage :\n",
    "class ClasseFille(NomDeLaClasse):\n",
    "  def __init__(self, parametre1, parametre2, parametre3):\n",
    "    NomDeLaClasse.__init__(self, parametre1, parametre2)\n",
    "    self.parametre3 = parametre3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "____\n",
    "## 3. Pr√©traitement de texte avec la biblioth√®que NLTK\n",
    "\n",
    "Le pr√©traitement du texte aide √† am√©liorer la qualit√© et la fiabilit√© des r√©sultats des analyses NLP en nettoyant et en normalisant les donn√©es textuelles. Cela peut √©galement acc√©l√©rer les analyses en r√©duisant la taille des donn√©es √† analyser.\n",
    "\n",
    "Le pr√©traitement du texte est une √©tape importante dans le traitement du langage naturel (NLP) pour plusieurs raisons :\n",
    "1. Conversion de la casse : La conversion de la casse peut √™tre importante pour garantir la coh√©rence des donn√©es et √©viter les probl√®mes li√©s √† la casse dans les algorithmes de NLP.\n",
    "\n",
    "\n",
    "2. Nettoyage de donn√©es : Les donn√©es textuelles brutes peuvent contenir des erreurs, des abr√©viations, des symboles et d'autres caract√©ristiques qui peuvent affecter la qualit√© des r√©sultats des analyses NLP. Le pr√©traitement du texte aide √† nettoyer ces donn√©es en supprimant les caract√®res ind√©sirables, en corrigeant les erreurs et en normalisant les donn√©es.\n",
    "\n",
    "3. Tokenisation : La tokenisation est l'√©tape cruciale de la division du texte en unit√©s plus petites pour une analyse ult√©rieure. La tokenisation peut √™tre utilis√©e pour diviser les donn√©es textuelles en mots, phrases, symboles et autres unit√©s pertinentes.\n",
    "\n",
    "4. Suppression des Stop words : Les stop words peuvent affecter n√©gativement les performances des algorithmes NLP en ajoutant du bruit aux donn√©es. La suppression des stop words aide √† filtrer les donn√©es et √† am√©liorer les r√©sultats de l'analyse.\n",
    "\n",
    "5. Stemming et Lemmatisation : Le stemming et la lemmatisation sont des techniques importantes pour normaliser les mots et les r√©duire √† leur forme de base pour une analyse plus coh√©rente.\n",
    "\n",
    "Nous utiliserons par la suite le vocabulaire suivant :\n",
    "- **Corpus** : Un corpus est un ensemble de documents textuels rassembl√©s en vu d'un traitement. \n",
    "- **Document** : Document : Un document est une unit√© de texte distincte, telle qu'un livre, un article de journal ou une page web. \n",
    "- **Text** : Le texte est un ensemble de mots et de phrases utilis√©s pour communiquer des informations et des id√©es. \n",
    "- **Token** : Un token est une unit√© d'information dans un texte, qui peut √™tre un mot, un symbole, une poctuation ou tout autre √©l√©ment pertinent.\n",
    "- **Vocabulaire** : C'est l'ensemble des tokens individuels pr√©sents dans l'ensemble du corpus.\n",
    "\n",
    "\n",
    "\n",
    "### 3.1 Traitement des chaines de caract√®re\n",
    "\n",
    "- `str.capitalize()` : retourne la premi√®re lettre en majuscule et le reste en minuscule.\n",
    "- `str.upper()` : retourne la cha√Æne de caract√®res en question en majuscule.\n",
    "- `str.lower()` : retourne la cha√Æne de caract√®res en question en minuscule.\n",
    "- `str.count()` : retourne le nombre d'occurrences d'une sous-cha√Æne dans la cha√Æne de caract√®res en question.\n",
    "- `str.find()` : retourne l'index de la premi√®re occurrence d'une sous-cha√Æne dans la cha√Æne de caract√®res en question. Si la sous-cha√Æne n'est pas trouv√©e, la m√©thode retourne -1.\n",
    "- `str.index()` : retourne l'index de la premi√®re occurrence d'une sous-cha√Æne dans la cha√Æne de caract√®res en question. Si la sous-cha√Æne n'est pas trouv√©e, la m√©thode l√®ve une exception ValueError.\n",
    "- `str.replace()` : retourne une nouvelle cha√Æne de caract√®res dans laquelle toutes les occurrences d'une sous-cha√Æne sont remplac√©es par une autre sous-cha√Æne.\n",
    "- `str.split()` : retourne une liste de cha√Ænes de caract√®res s√©par√©es par un s√©parateur sp√©cifi√©. Si le s√©parateur n'est pas sp√©cifi√©, la m√©thode utilise par d√©faut l'espace.\n",
    "- `str.strip()` : renvoie une copie de la cha√Æne sans les espaces en d√©but et fin.\n",
    "- `str.isdigit()` : retourne True si tous les caract√®res de la cha√Æne sont des chiffres, sinon False.\n",
    "- `str.isalpha()` : retourne True si tous les caract√®res de la cha√Æne sont des lettres, sinon False.\n",
    "- `str.isalnum()` : retourne True si tous les caract√®res de la cha√Æne sont des chiffres ou des lettres, sinon Flase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['isupper',\n",
       " 'join',\n",
       " 'ljust',\n",
       " 'lower',\n",
       " 'lstrip',\n",
       " 'maketrans',\n",
       " 'partition',\n",
       " 'removeprefix',\n",
       " 'removesuffix',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'rjust',\n",
       " 'rpartition',\n",
       " 'rsplit',\n",
       " 'rstrip',\n",
       " 'split',\n",
       " 'splitlines',\n",
       " 'startswith',\n",
       " 'strip',\n",
       " 'swapcase',\n",
       " 'title',\n",
       " 'translate',\n",
       " 'upper',\n",
       " 'zfill']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Text preprocessing helps improve the quality and reliability of NLP analysis results by cleaning and normalizing textual data, It's amazing !\n",
    "It can also speed up analyzes by reducing the size of the data to be analyzed.\"\"\"\n",
    "\n",
    "dir(text)[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le texte est-il digital : False\n",
      "Le texte est-il uniquement alphabetique : False\n",
      "Le texte est-il en majuscule : False\n",
      "Le texte est-il en minuscule : False      \n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Le texte est-il digital : {text.isdigit()}\n",
    "Le texte est-il uniquement alphabetique : {text.isalpha()}\n",
    "Le texte est-il en majuscule : {text.isupper()}\n",
    "Le texte est-il en minuscule : {text.islower()}      \n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lower :  text preprocessing helps improve the quality and reliability of nlp analysis results by cleaning and normalizing textual data, it's amazing !\n",
      "it can also speed up analyzes by reducing the size of the data to be analyzed.\n",
      "\n",
      "Text uper :  TEXT PREPROCESSING HELPS IMPROVE THE QUALITY AND RELIABILITY OF NLP ANALYSIS RESULTS BY CLEANING AND NORMALIZING TEXTUAL DATA, IT'S AMAZING !\n",
      "IT CAN ALSO SPEED UP ANALYZES BY REDUCING THE SIZE OF THE DATA TO BE ANALYZED.\n",
      "\n",
      "Text Capital :  Text preprocessing helps improve the quality and reliability of nlp analysis results by cleaning and normalizing textual data, it's amazing !\n",
      "it can also speed up analyzes by reducing the size of the data to be analyzed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Text lower : ', text.lower())\n",
    "print()\n",
    "print('Text uper : ', text.upper())\n",
    "print()\n",
    "print('Text Capital : ', text.capitalize())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequence du token \"the\" :  3\n",
      "\n",
      "Position du token \"the\" :  33\n",
      "\n",
      "Position du token \"the\" :  184\n",
      "\n",
      "Position du token \"the\" :  Text preprocessing helps improve **** quality and reliability of NLP analysis results by cleaning and normalizing textual data, It's amazing !\n",
      "It can also speed up analyzes by reducing **** size of **** data to be analyzed.\n"
     ]
    }
   ],
   "source": [
    "print('Frequence du token \"the\" : ', text.count('the'))\n",
    "print()\n",
    "print('Position du token \"the\" : ', text.find('the'))\n",
    "print()\n",
    "print('Position du token \"the\" : ', text.index('the', 50))\n",
    "print()\n",
    "print('Position du token \"the\" : ', text.replace('the', '****'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tokenisation\n",
    "\n",
    "<img src='https://miro.medium.com/max/1050/0*EKgminT7W-0R4Iae.png'>\n",
    "\n",
    "La tokenisation est un processus dans le traitement du langage naturel (NLP) qui consiste √† diviser un texte en unit√©s plus petites appel√©es tokens. \n",
    "\n",
    "Les tokens peuvent √™tre des mots, des phrases, des symboles ou des caract√®res. La tokenisation est souvent la premi√®re √©tape dans le traitement des donn√©es textuelles, car elle permet de pr√©parer le texte pour les analyses ult√©rieures telles que la reconnaissance de la signification, la classification, la g√©n√©ration de r√©sum√©s, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'helps',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'and',\n",
       " 'reliability',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'analysis',\n",
       " 'results',\n",
       " 'by',\n",
       " 'cleaning',\n",
       " 'and',\n",
       " 'normalizing',\n",
       " 'textual',\n",
       " 'data,',\n",
       " \"It's\",\n",
       " 'amazing',\n",
       " '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = text.split('\\n')[0]\n",
    "tokens = document.split(' ')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) est une biblioth√®que Python d√©di√©e au traitement du langage naturel (NLP). Il s'agit d'un outil de r√©f√©rence pour les d√©veloppeurs et les chercheurs travaillant dans le domaine du NLP. \n",
    "\n",
    "Cette biblioth√®que fournit une vari√©t√© de fonctionnalit√©s pour les t√¢ches courantes de NLP :\n",
    "-  telles que la tokenisation, \n",
    "- la reconnaissance d'entit√©s nomm√©es, \n",
    "- la g√©n√©ration de textes, l'analyse de sentiments, \n",
    "- la classification de textes,\n",
    "- le support de plusieurs langues dont le fran√ßais\n",
    "- la gestion des `stop words`\n",
    "\n",
    "Installation de la biblioth√®que NLTK : \n",
    "- sur Windows : `pip install nltk`\n",
    "- sur MacOs : `pip3 install nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 16.0 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     ------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp311-cp311-win_amd64.whl (268 kB)\n",
      "     ------------------------------------- 268.3/268.3 kB 17.2 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\admin-stagiaire\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 regex-2023.8.8 tqdm-4.66.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installation de la biblioth√®que √† partir de l'environement Jupyter\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T√©l√©chargement des d√©pendance NLTP\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'helps',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'and',\n",
       " 'reliability',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'analysis',\n",
       " 'results',\n",
       " 'by',\n",
       " 'cleaning',\n",
       " 'and',\n",
       " 'normalizing',\n",
       " 'textual',\n",
       " 'data',\n",
       " ',',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'amazing',\n",
       " '!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the tokenizer of NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize our sentence\n",
    "tokens = word_tokenize(document)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Traitement des StopWords\n",
    "\n",
    "\n",
    "Un stop word d√©signe des mots qui sont souvent ignor√©s ou filtr√©s lors de l'analyse de donn√©es textuelles. Les stop words sont consid√©r√©s comme des mots peu informatifs et ne sont g√©n√©ralement pas pris en compte lors de l'analyse de la signification des textes. \n",
    "\n",
    "Ils peuvent √™tre des pr√©positions, des conjonctions et d'autres mots communs qui ne contribuent pas de mani√®re significative √† la signification d'une phrase ou d'un document. \n",
    "\n",
    "La liste des stop words peut varier en fonction de la langue, du domaine et des objectifs de l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de stop words : 179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print('Nombre de stop words : {}'.format(len(stop_words)))\n",
    "stop_words[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Supprimons ces mots de nos jetons en utilisant NLTK :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'helps',\n",
       " 'improve',\n",
       " 'quality',\n",
       " 'reliability',\n",
       " 'NLP',\n",
       " 'analysis',\n",
       " 'results',\n",
       " 'cleaning',\n",
       " 'normalizing',\n",
       " 'textual',\n",
       " 'data',\n",
       " ',',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'amazing',\n",
       " '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_no_stops = [t for t in tokens if t not in stop_words]\n",
    "tokens_no_stops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter que le mot : `And` n'ont pas √©t√© supprim√©s car python est sensible √† la casse :\n",
    "\n",
    "`'And' != 'and' `\n",
    "\n",
    "Veillez √† bien mettre en minuscule votre text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stemming & Lemmatization\n",
    "\n",
    "<img src='https://d2mk45aasx86xg.cloudfront.net/difference_between_Stemming_and_lemmatization_8_11zon_452539721d.webp'>\n",
    "\n",
    "**Stemming** et **lemmatisation** sont deux techniques utilis√©es dans le traitement du langage naturel pour r√©duire les mots √† leur forme de base.\n",
    "\n",
    "**Stemming** : Le stemming est une technique pour extraire la racine d'un mot en enlevant les suffixes, les pr√©fixes et autres modifications morphologiques. L'objectif du stemming est de r√©duire les mots √† leur forme de base pour une analyse plus coh√©rente. Par exemple, les mots \"runner\", \"running\", \"ran\" peuvent √™tre r√©duits √† la forme de base \"run\".\n",
    "\n",
    "**Lemmatisation** : La lemmatisation est similaire au stemming, mais elle vise √† produire un lemme ou un mot normalis√©, qui est une forme valide de dictionnaire pour un mot donn√©. La lemmatisation implique une analyse morphologique plus avanc√©e pour d√©terminer la forme correcte d'un mot, en prenant en compte son contexte et sa d√©finition. Par exemple, le mot \"running\" peut √™tre lemmatis√© en \"run\", tandis que le mot \"better\" peut √™tre lemmatis√© en \"good\".\n",
    "\n",
    "En g√©n√©ral, la lemmatisation est consid√©r√©e comme une technique plus pr√©cise que le stemming, mais elle est √©galement plus lente et plus complexe √† impl√©menter. Les deux techniques peuvent √™tre utiles pour normaliser les mots et am√©liorer les r√©sultats des analyses NLP, mais le choix entre les deux d√©pend des besoins sp√©cifiques d'un projet NLP particulier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries for stemming and lemmatization\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Then we have to create an instance\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce faire, nous devons d√©finir quel type de mot nous lemmatisons : 'n' pour le nom, 'v' pour le verbe, 'a' pour l'adjectif et 'r' pour les adverbes. La valeur par d√©faut est 'n' pour le nom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect connect connect\n",
      "mean mean\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('connection'), stemmer.stem('connected'), stemmer.stem('connective'))\n",
    "\n",
    "# but what if the words don't mean the same thing once truncated\n",
    "print(stemmer.stem('meaning'), stemmer.stem('meanness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa be was was was\n",
      "\n",
      "changed change changed changed changed\n",
      "\n",
      "connected connect connected connected connected\n",
      "\n",
      "meaning mean meaning meaning meaning\n",
      "\n",
      "changing change changing changing changing\n",
      "\n",
      "wording word wording wording wording\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in ['was', 'changed','connected', 'meaning', 'changing', \"wording\"]:\n",
    "    print(lemmatizer.lemmatize(word, 'n'), \n",
    "        lemmatizer.lemmatize(word, 'v'), \n",
    "        lemmatizer.lemmatize(word, 'a'),\n",
    "        lemmatizer.lemmatize(word, 's'),\n",
    "        lemmatizer.lemmatize(word, 'r'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed words :  ['text', 'preprocess', 'help', 'improv', 'qualiti', 'reliabl', 'nlp', 'analysi', 'result', 'clean', 'normal', 'textual', 'data', '.']\n",
      "\n",
      "lemmatized words :  ['Text', 'preprocessing', 'help', 'improve', 'the', 'quality', 'and', 'reliability', 'of', 'NLP', 'analysis', 'result', 'by', 'cleaning', 'and', 'normalizing', 'textual', 'data.']\n"
     ]
    }
   ],
   "source": [
    "stem  = [stemmer.stem(word) for word in processing.data_token[1][1]]\n",
    "\n",
    "lem = [lemmatizer.lemmatize(word, 'n') for word in document.split()]\n",
    "\n",
    "print(\"stemmed words : \", stem)\n",
    "print()\n",
    "print(\"lemmatized words : \", lem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Ngrammes\n",
    "\n",
    "Jusqu'√† pr√©sent, nous n'avons utilis√© que des unigrammes de mots. Mais parfois, on peut vouloir utiliser aussi des bigrammes , voire des trigrammes de mots. Ou m√™me des unigrammes, des bigrammes et des trigrammes de caract√®res, pourquoi pas ?\n",
    "\n",
    "üëâüèª Voyons sur un exemple simple ce que seraient des bigrammes de la phrase suivante : ¬´ √ätre ou ne pas √™tre ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'preprocessing', 'helps', 'improve', 'the', 'quality', 'and', 'reliability', 'of', 'NLP', 'analysis', 'results', 'by', 'cleaning', 'and', 'normalizing', 'textual', 'data', '.'] \n",
      "\n",
      "bigrams: [('Text', 'preprocessing'), ('preprocessing', 'helps'), ('helps', 'improve'), ('improve', 'the'), ('the', 'quality'), ('quality', 'and'), ('and', 'reliability'), ('reliability', 'of'), ('of', 'NLP'), ('NLP', 'analysis'), ('analysis', 'results'), ('results', 'by'), ('by', 'cleaning'), ('cleaning', 'and'), ('and', 'normalizing'), ('normalizing', 'textual'), ('textual', 'data'), ('data', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Import the module ngrams\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Print the bigrams and trigrams\n",
    "print(tokens, '\\n')\n",
    "\n",
    "print('bigrams:', list(ngrams(tokens, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Text preprocessing helps improve the quality and reliability of NLP analysis results by cleaning and normalizing textual data.',\n",
       "  ['text',\n",
       "   'preprocessing',\n",
       "   'helps',\n",
       "   'improve',\n",
       "   'quality',\n",
       "   'reliability',\n",
       "   'nlp',\n",
       "   'analysis',\n",
       "   'results',\n",
       "   'cleaning',\n",
       "   'normalizing',\n",
       "   'textual',\n",
       "   'data',\n",
       "   '.']),\n",
       " ('Text preprocessing helps improve the quality and reliability of NLP analysis results by cleaning and normalizing textual data.',\n",
       "  ['text',\n",
       "   'preprocessing',\n",
       "   'help',\n",
       "   'improve',\n",
       "   'quality',\n",
       "   'reliability',\n",
       "   'nlp',\n",
       "   'analysis',\n",
       "   'result',\n",
       "   'clean',\n",
       "   'normalize',\n",
       "   'textual',\n",
       "   'data',\n",
       "   '.'])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing.data_lem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercices\n",
    "\n",
    "1. Cre√©ez une classe `Processing` contenant une m√©thode `tokenization` qui tranfome un document en liste de token.\n",
    "Cette m√©thode poss√®de 3 arguments :\n",
    "- `document : str`  --> Le document sous forme de str,\n",
    "- `stem:bool=False` --> Si True, la m√©thode applique la transformation stemming,\n",
    "- `lem:bool=False` --> Si True, la m√©thode applique la transformation lemmatization.\n",
    "\n",
    "La m√©thode garde dans un attribut data, l'ensemble des pr√©c√©dents traitement : le document et la liste des tokens.\n",
    "\n",
    "La m√©thode retourne la liste de token.\n",
    "\n",
    "\n",
    "2. Testez votre programme sur le jeu de donn√©es [suivant](https://drive.google.com/file/d/1Pz9YfRErwnkgD2qTk4Q0CCY_PPP7akqa/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class Processing:\n",
    "    def __init__(self):\n",
    "        self.test = {\n",
    "            \"document\": None,\n",
    "            \"tokens\": None,\n",
    "        }\n",
    "\n",
    "    def tokenization(self, document, stem=False , lem=False):\n",
    "        self.test[\"document\"] = document\n",
    "\n",
    "        tokens = word_tokenize(document)\n",
    "     \n",
    "\n",
    "        if stem:\n",
    "            stemmer = PorterStemmer()\n",
    "            tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        if lem:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        self.test[\"tokens\"] = tokens\n",
    "        return tokens\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document original: Ceci est un test.\n",
      "Tokens trait√©s: ['ceci', 'est', 'un', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "processor = Processing()\n",
    "document = \"Ceci est un test.\"\n",
    "    \n",
    "tokens = processor.tokenization(document, stem=True, lem=True)\n",
    "print(\"Document original:\", processor.test[\"document\"])\n",
    "print(\"Tokens trait√©s:\", processor.test[\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
